# 基于弱暗纹理的对抗样本检测

## 纹理

**纹理**是影像中大量规律性很强或很弱的相似元素或者图形结构，一般理解为**影像灰度**在空间上的变化和重复,或影像中反复出现的局部模式(纹理单元)和它们的排列规则。

## 对抗样本

### 术语

白盒/黑盒攻击：对目标模型的先验知识

目标/非目标攻击：是否有特定目标结果

单步/迭代攻击：生成对抗样本的计算复杂度

置信度：样本被分类为某种类别的概率

类别标签：图片分类得到的标签结果

目标模型：被攻击模型

替代模型：敌手设法训练的模型

扰动约束度量：通过范数定义图像扰动的差异

### 攻击方法

#### 基于梯度

要利用目标模型关于给定 输入的梯度信息来寻找一个使模型损失值更大的对抗 扰动，从而使加入该对抗扰动的正常图像导致模型误 分类

**FGSM方法**：沿正常样本梯度的反方向添加扰动

**I-FGSM方法**：将 FGSM 的单次计算对抗扰动转换为迭代小步计算对抗扰动

**PGD**：使用均匀的随机噪声初始化，增 加攻击的迭代轮数

![image-20220922234856791](C:\Users\faii\AppData\Roaming\Typora\typora-user-images\image-20220922234856791.png)

### 基于优化

**L-BFGS**：通过寻找导致神经网络误分类的 最小损失函数加性扰动项，将问题转化为凸优化问题， 形式化为公式所示的优化问题来寻找对抗扰动

### 基于超平面

将正常图像周围的类边界线性化， 形成一个凸多面体，然后向最优方向更新一小步，将 正常图像推向最近的分类超平面，直到其跨过分类超 平面改变类标签

### 基于迁移（黑盒）

基于迁移的攻击允许攻击者进行目标模型的查询 和访问目标模型的一部分训练数据集，然后攻击者使 用这些信息构建一个合成模型，攻击者在合成模型上 使用白盒攻击生成对抗样本，最后将该对抗样本迁移 到目标模型上进行攻击。

**LSMA**：击通过生成替代模型(Substitute Model) 来模拟被攻击模型的近似决策边界，并基于当前的替 代模型生成对抗样本，这些对抗样本最终被用于攻击 原始目标模型

### 基于置信度分数查询

基于置信度分数查询的攻击相对于基于迁移的攻 击拥有更强的假设，不需要任何关于数据集的知识， 它会反复查询看不见的分类器，得到分类器输出的置 信度向量，以尝试生成合适的对抗扰动来完成攻击。

## 防御方法

### 增强模型鲁棒性的防御

对抗训练是让模型在训练过程中暴露在对抗样本中，以获得对对抗样本的 免疫力。其它的增强鲁棒性的防御，通 过正常的训练数据改变模型的 相关结构，例如使用新型损失函数替换经典的交叉熵损失函 数

### 输入预处理的防御

对输入模型的数据样本进行压缩、变换等操作来缓解对抗扰动的影响；对输入模型的数据样本进行去噪处理来消除对抗扰动

### 对抗样本的检测防御

<- 这次prp的方向大概属于这里，重点看看

为预先训练的模型提供相应的机制或者模块，来检测对抗样本，以保护模型免受对抗攻击

![image-20220923000558554](C:\Users\faii\AppData\Roaming\Typora\typora-user-images\image-20220923000558554.png)



Detecting adversarial examples in deep networks with adaptive noise reduction

### 可验证鲁棒性的防御

