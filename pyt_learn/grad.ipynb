{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，区分是否为线性模型，主要是看一个乘法式中自变量前的系数w，若w只影响一个自变量（或决策边界为线性），则为线性模型，而一旦有自变量被超过一个参数影响，则为非线性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**机器学习模型训练的步骤**:\n",
    "\n",
    "数据模块（数据采集，清洗，处理等）\n",
    "\n",
    "建立模型（各种模型的建立）\n",
    "\n",
    "损失函数的选择（根据不同的任务选择不同的损失函数），有了loss就可以求取梯度\n",
    "\n",
    "得到梯度之后，我们会选择某种优化方式去进行优化\n",
    "\n",
    "然后迭代训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"数据生成\"\"\"\n",
    "torch.manual_seed(1)\n",
    "\n",
    "sample_nums = 100\n",
    "mean_value = 1.7\n",
    "bias = 1\n",
    "\n",
    "n_data = torch.ones(sample_nums, 2) # 100行，2列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.normal(mean_value*n_data, 1) + bias  # 类别0  数据shape=(100,2)\n",
    "y0 = torch.zeros(sample_nums)   # 类别0， 数据shape=(100, 1)\n",
    "x1 = torch.normal(-mean_value*n_data, 1) + bias   # 类别1， 数据shape=(100,2)\n",
    "y1 = torch.ones(sample_nums)    # 类别1  shape=(100, 1)\n",
    "\n",
    "train_x = torch.cat([x0, x1], 0)\n",
    "train_y = torch.cat([y0, y1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"建立模型\"\"\"\n",
    "class LR(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR, self).__init__()\n",
    "        self.features = torch.nn.Linear(2, 1)  # Linear 是module的子类，是参数化module的一种，与其名称一样，表示着一种线性变换。输入2个节点，输出1个节点\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x) # linear feature\n",
    "        x = self.sigmoid(x) # sigmoid function\n",
    "        \n",
    "        return x\n",
    "\n",
    "lr_net = LR()     # 实例化逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"选择损失函数\"\"\"\n",
    "loss_fn = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"选择优化器\"\"\"\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"模型训练\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for iteration in range(1000):\n",
    "    \n",
    "    # 前向传播\n",
    "    y_pred = lr_net(train_x)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = loss_fn(y_pred.squeeze(), train_y)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 绘图\n",
    "    if iteration % 20 == 0:\n",
    "\n",
    "        mask = y_pred.ge(0.5).float().squeeze()  # 以0.5为阈值进行分类\n",
    "        correct = (mask == train_y).sum()  # 计算正确预测的样本个数\n",
    "        acc = correct.item() / train_y.size(0)  # 计算分类准确率\n",
    "\n",
    "        plt.scatter(x0.data.numpy()[:, 0], x0.data.numpy()[:, 1], c='r', label='class 0')\n",
    "        plt.scatter(x1.data.numpy()[:, 0], x1.data.numpy()[:, 1], c='b', label='class 1')\n",
    "\n",
    "        w0, w1 = lr_net.features.weight[0]\n",
    "        w0, w1 = float(w0.item()), float(w1.item())\n",
    "        plot_b = float(lr_net.features.bias[0].item())\n",
    "        plot_x = np.arange(-6, 6, 0.1)\n",
    "        plot_y = (-w0 * plot_x - plot_b) / w1\n",
    "\n",
    "        plt.xlim(-5, 7)\n",
    "        plt.ylim(-7, 7)\n",
    "        plt.plot(plot_x, plot_y)\n",
    "\n",
    "        plt.text(-5, 5, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.title(\"Iteration: {}\\nw0:{:.2f} w1:{:.2f} b: {:.2f} accuracy:{:.2%}\".format(iteration, w0, w1, plot_b, acc))\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "\n",
    "        if acc > 0.99:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先基于前面的张量的知识我们又更进一步，学习了计算图的机制，计算图说白了就是描述运算过程的图， 有了这个图梯度求导的时候非常方便。 然后学习了Pytorch的动态图机制，区分了一下动态图和静态图。 然后学习了Pytorch的自动求导机制，认识了两个比较常用的函数torch.autograd.backward()和torch.autograd.grad()函数， 关于自动求导要记得三个注意事项： 梯度手动清零，叶子节点不能原位操作，依赖于叶子节点的节点默认是求梯度。 最后我们根据上面的所学知识建立了一个逻辑回归模型实现了一个二分类的任务\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8735e5a44aaffc82247471f98e61afd3a8c083af1d83bb5f2428c9ba5da5121f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
